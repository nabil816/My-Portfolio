{"cells":[{"cell_type":"markdown","metadata":{"id":"pnMPsv0t94gl"},"source":["# Our Data Sources"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":27,"status":"ok","timestamp":1755498269577,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"hVoR5KLf94pP"},"outputs":[],"source":["# Required Datasets\n","Sales_full = 'Sales_full'\n","Sales_mini = 'Sales_mini'\n","Current_price = 'Current_price'\n","COGs = 'COGs.xlsx'"]},{"cell_type":"markdown","metadata":{"id":"DLuaGMZr92Sl"},"source":["# Our Libraries"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":938,"status":"ok","timestamp":1755498270521,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"XyKGUxxJ92nr"},"outputs":[],"source":["# Standard libraries\n","import os\n","import io\n","import math\n","import calendar\n","import datetime\n","from os import listdir\n","from os.path import isfile, join\n","from datetime import date, datetime\n","from dateutil.relativedelta import relativedelta\n","\n","# Data manipulation & analysis\n","import pandas as pd\n","import numpy as np\n","\n","# Excel file handling\n","import openpyxl\n","from openpyxl import load_workbook\n","\n","# Google Colab environment\n","from google.colab import drive\n","from google.colab import auth\n","\n","# Google API Client\n","from googleapiclient.http import MediaIoBaseDownload\n","from googleapiclient.discovery import build\n","from googleapiclient.errors import HttpError\n","\n","# Google authentication\n","from google.auth import default\n","from google.oauth2 import service_account\n"]},{"cell_type":"markdown","metadata":{"id":"FMsS-RDaDx7o"},"source":["# Mount to our driver"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":2397,"status":"ok","timestamp":1755498272908,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"hDFsdveeD_c-","outputId":"2261dd14-937e-460f-c32e-8c88cb3b19f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"execute_result","data":{"text/plain":["'/content'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":3}],"source":["# First of all, we need to make sure that my directory is in My Drive\n","drive.mount('/content/drive')\n","os.getcwd()"]},{"cell_type":"markdown","metadata":{"id":"TcR2JIkF-Knh"},"source":["## Our Functions"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":38,"status":"ok","timestamp":1755498272938,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"EzBXOi5v92qG"},"outputs":[],"source":["def get_spreadsheet_id(url: str) -> str:\n","    \"\"\"\n","    Extracts the Google Spreadsheet ID from a given URL.\n","\n","    Args:\n","        url (str): Full Google Sheets URL.\n","\n","    Returns:\n","        str: Spreadsheet ID.\n","\n","    Raises:\n","        ValueError: If the URL is invalid or ID cannot be extracted.\n","    \"\"\"\n","    try:\n","        return url.split(\"/d/\")[1].split(\"/\")[0]\n","    except (IndexError, AttributeError):\n","        raise ValueError(\"Invalid Google Sheets URL provided.\")"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":25,"status":"ok","timestamp":1755498272968,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"joysInMQ-RR7"},"outputs":[],"source":["def get_information_about_file_in_drive(file_id):\n","  # Get default credentials and build the Drive service\n","  auth.authenticate_user()\n","  creds, _ = default()\n","  drive_service = build('drive', 'v3', credentials=creds)\n","  # get the spreadsheet ID\n","  file_id = get_spreadsheet_id(url)\n","  # Try to access the file metadata\n","  try:\n","      file = drive_service.files().get(fileId=file_id, fields=\"id, name, mimeType, owners\").execute()\n","      print(f\"âœ… You have access to the file: {file['name']}\")\n","      print(f\"ðŸ“„ File ID: {file['id']}\")\n","      print(f\"ðŸ‘¤ Owner: {file['owners'][0]['emailAddress']}\")\n","      print(f\"ðŸ“ Type: {file['mimeType']}\")\n","  except HttpError as e:\n","      if e.resp.status == 404:\n","            print(\"âŒ File not found.\")\n","      elif e.resp.status == 403:\n","          print(\"âŒ You do NOT have access to this file.\")\n","      else:\n","        print(f\"âš ï¸ Error occurred: {e}\")"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":51,"status":"ok","timestamp":1755498273015,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"2xr-UfbY-Tju"},"outputs":[],"source":["def get_sheet_names(file_path: str):\n","    \"\"\"\n","    Get all sheet names from a local Excel file.\n","    \"\"\"\n","    try:\n","        # Load the Excel file\n","        xlsx = pd.ExcelFile(file_path)\n","\n","        # Get all sheet names\n","        sheet_names = xlsx.sheet_names\n","\n","        # print the sheet names\n","        print(sheet_names)\n","    except Exception as e:\n","        print(f\"Error reading file: {e}\")\n","        return []\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1755498273032,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"nZiGBusUUR23"},"outputs":[],"source":["def Get_Previous_Months_With_Year(num_of_previous_months: int = 3) -> dict:\n","    \"\"\"\n","    Generate a dictionary of previous months with their corresponding years.\n","\n","    This function calculates the month names and years for a given number of\n","    months preceding the current date. The results are stored in a dictionary\n","    where:\n","        - Keys are full month names (e.g., \"July\").\n","        - Values are the corresponding years (e.g., 2025).\n","\n","    Parameters\n","    ----------\n","    num_of_previous_months : int, optional\n","        Number of past months to include (default is 3).\n","\n","    Returns\n","    -------\n","    dict\n","        Dictionary of month names (str) mapped to years (int).\n","        Example: {'July': 2025, 'June': 2025, 'May': 2025}\n","\n","    Examples\n","    --------\n","    >>> Get_Previous_Months_With_Year(3)\n","    {'July': 2025, 'June': 2025, 'May': 2025}\n","\n","    >>> Get_Previous_Months_With_Year(5)\n","    {'July': 2025, 'June': 2025, 'May': 2025, 'April': 2025, 'March': 2025}\n","    \"\"\"\n","    now = datetime.now()\n","    months = {}\n","\n","    for i in range(1, num_of_previous_months + 1):\n","        past_date = now - relativedelta(months=i)\n","        month_name = past_date.strftime(\"%B\")\n","        year = past_date.year\n","        months[month_name] = year\n","\n","    return months\n"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1755498273047,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"xF9HYuSYUSnh"},"outputs":[],"source":["def filter_df_by_previous_months(df, num_of_previous_months = 3):\n","    \"\"\"\n","    Filters the input DataFrame to include only rows from the previous three months.\n","\n","    Args:\n","        df_full (pd.DataFrame): The full dataset containing at least 'Month' and 'Year' columns.\n","\n","    Returns:\n","        pd.DataFrame: Filtered DataFrame with rows from the previous three months.\n","    \"\"\"\n","    dict_month_year = Get_Previous_Months_With_Year(num_of_previous_months)\n","    df_previous_months = pd.DataFrame()\n","\n","    for month, year in dict_month_year.items():\n","        df_t = df[(df['Month'] == month) & (df['Year'] == year)]\n","        df_previous_months = pd.concat([df_previous_months, df_t], ignore_index=True)\n","\n","    return df_previous_months\n"]},{"cell_type":"code","source":["def get_unmapped_marketplaces(df, marketplace_col, mapping_dict, with_counts=False):\n","    \"\"\"\n","    Find unmapped marketplaces in a DataFrame compared to a mapping dictionary.\n","\n","    Parameters\n","    ----------\n","    df : pandas.DataFrame\n","        DataFrame that contains marketplace data.\n","    marketplace_col : str\n","        Name of the column in df that stores marketplace values.\n","    mapping_dict : dict\n","        Dictionary mapping marketplace names to countries (or other values).\n","    with_counts : bool, optional\n","        If True, returns a dictionary of unmapped marketplaces with their counts.\n","        If False, returns only a set of unmapped marketplaces.\n","\n","    Returns\n","    -------\n","    set or dict\n","        - If with_counts=False: set of unmapped marketplaces.\n","        - If with_counts=True: dict {marketplace: count}.\n","    \"\"\"\n","    unique_marketplaces = set(df[marketplace_col].unique())\n","    mapped_marketplaces = set(mapping_dict.keys())\n","\n","    # Difference â†’ marketplaces in df but not in dict\n","    unmapped_marketplaces = unique_marketplaces - mapped_marketplaces\n","\n","    if with_counts:\n","        counts = (\n","            df[df[marketplace_col].isin(unmapped_marketplaces)][marketplace_col]\n","            .value_counts()\n","            .to_dict()\n","        )\n","        return counts\n","    else:\n","        return unmapped_marketplaces\n"],"metadata":{"id":"hhvq_9R1tFqc","executionInfo":{"status":"ok","timestamp":1755498273057,"user_tz":-180,"elapsed":5,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eNQZ5MZQ_Td1"},"source":["# Key Dictionaries for Important Data Mapping\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1755498273068,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"wVbaw5Tw_194"},"outputs":[],"source":["dict_maketplace_country = { 'Amazon.co.uk': \"UK\",\n","                           'Amazon.de':'EU',\n","                            'Amazon.fr': 'EU',\n","                            'Amazon.es': 'EU',\n","                            'Amazon.it': 'EU',\n","                            'Amazon.nl': 'EU',\n","                            'Amazon.com.be': 'EU',\n","                            'Amazon.ie': 'EU',\n","                            'Amazon.pl':'EU',\n","                            'Amazon.com.tr': 'EU',\n","                            'Amazon.se': 'EU',\n","                            'Amazon.ae': 'EU',\n","                            'Amazon.com': 'US',\n","                            'Amazon.ca': 'CA',\n","                            'Amazon.com.mx': 'MX',\n","                            'Amazon.co.jp': 'JP'}\n","\n","\n","\n","\n","\n","LTSF_Threshold= {'US':181, 'CA':181 ,'UK': 241,  'EU': 241, 'JP':271}\n"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":40,"status":"ok","timestamp":1755498273126,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"},"user_tz":-180},"id":"owxL0mcT_Ty2"},"outputs":[],"source":["USA_columns = [(                       ' ',                      'Products'),\n","            (                     'USA',                           'SKU'),\n","            (                     'USA',                          'ASIN'),\n","            (           'ON HAND STOCK',        'Average Sales (7 days)'),\n","            (           'ON HAND STOCK',       'Average Sales (30 days)'),\n","            (           'ON HAND STOCK',               'Units in Amazon'),\n","            (           'ON HAND STOCK',       'Transit to AMZ from 3PL'),\n","            (           'ON HAND STOCK',                  'Units in AWD'),\n","            (           'ON HAND STOCK',               'Stocks with 3PL'),\n","            (           'ON HAND STOCK',                   'TOTAL UNITS')]\n","\n","\n","CA_columns =  [(      'Unnamed: 0_level_0',                   'Products'),\n","            (                      'CA',                        'SKU'),\n","            (                      'CA',                       'ASIN'),\n","            (           'Average Sales',                     '7 days'),\n","            (           'Average Sales',                    '30 days'),\n","            (           'ON HAND STOCK',            'Units in Amazon'),\n","            (           'ON HAND STOCK',   'Transit to AMZ from 3 PL'),\n","            (           'ON HAND STOCK',            'Stocks with 3PL'),\n","            (           'ON HAND STOCK',                'TOTAL UNITS')]\n","\n","\n","UK_columns = [(      'Unnamed: 0_level_0',                   'Products'),\n","            (                      'UK',                        'SKU'),\n","            (                      'UK',                       'ASIN'),\n","            (           'Average Sales',                     '7 days'),\n","            (           'Average Sales',                    '30 days'),\n","            (           'ON HAND STOCK',            'Units in Amazon'),\n","            (           'ON HAND STOCK',   'Transit to AMZ from 3 PL'),\n","            (           'ON HAND STOCK',            'Stocks with 3PL'),\n","            (           'ON HAND STOCK',                'TOTAL UNITS')]\n","\n","\n","EU_columns = [(      'Unnamed: 0_level_0',                    'PRODUCT'),\n","            (                      'EU',                        'SKU'),\n","            (                      'EU',                       'ASIN'),\n","            (           'Average Sales',                     '7 days'),\n","            (           'Average Sales',                    '30 days'),\n","            (           'ON HAND STOCK',            'Units in Amazon'),\n","            (           'ON HAND STOCK',   'Transit to AMZ from 3 PL'),\n","            (           'ON HAND STOCK',            'Stocks with 3PL'),\n","            (           'ON HAND STOCK',                'TOTAL UNITS')]\n","\n","\n","\n","JP_columns = [(      'Unnamed: 0_level_0',                    'PRODUCT'),\n","            (                   'JAPAN',                        'SKU'),\n","            (                   'JAPAN',                       'ASIN'),\n","            (           'Average Sales',                     '7 days'),\n","            (           'Average Sales',                    '30 days'),\n","            (           'ON HAND STOCK',            'Units in Amazon'),\n","            (           'ON HAND STOCK',   'Transit to AMZ from 3 PL'),\n","            (           'ON HAND STOCK',            'Stocks with 3PL'),\n","            (           'ON HAND STOCK',                'TOTAL UNITS')]\n","\n","\n","\n","\n","\n","\n","\n","columns_names_usa = ['Products', 'SKU', 'ASIN', 'Average Sales (7 days)', 'Average Sales (30 days)', 'Units in Amazon', 'Transit to AMZ from 3PL',  'Units in AWD',  'Stocks with 3PL', 'TOTAL UNITS']\n","columns_names_other_countries = ['Products', 'SKU', 'ASIN', 'Average Sales (7 days)', 'Average Sales (30 days)', 'Units in Amazon', 'Transit to AMZ from 3PL', 'Stocks with 3PL', 'TOTAL UNITS']"]},{"cell_type":"markdown","metadata":{"id":"5GxHnoSl-gzM"},"source":["# Getting our Data\n"]},{"cell_type":"markdown","metadata":{"id":"kEKHYSHBPktb"},"source":["## Getting the active cells in the full stock file"]},{"cell_type":"markdown","source":[">> We get this data from the full stock file and I have edit access to it"],"metadata":{"id":"Oln5Hu8_fQnO"}},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ns6ZYXvu-gFt","executionInfo":{"status":"ok","timestamp":1755498357725,"user_tz":-180,"elapsed":84593,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}},"outputId":"a2f10aa4-53ad-4a9f-888e-d148532cdce2"},"outputs":[{"output_type":"stream","name":"stdout","text":["âœ… You have access to the file: Full stocks count.xlsx\n","ðŸ“„ File ID: 1fGDIkxG_QDuQtZUKssmz399nWGntz56O\n","ðŸ‘¤ Owner: annie@tilcotrading.com\n","ðŸ“ Type: application/vnd.openxmlformats-officedocument.spreadsheetml.sheet\n","ðŸ”„ Download progress: 100%\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","/usr/local/lib/python3.11/dist-packages/openpyxl/worksheet/_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n","  warn(msg)\n","/tmp/ipython-input-3976809241.py:97: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  df_fs.fillna(0,inplace = True) # Replace NaN with 0\n"]}],"source":["# -------------------------------\n","# Step 1. Get File Information\n","# -------------------------------\n","\n","url = \"https://docs.google.com/spreadsheets/d/1fGDIkxG_QDuQtZUKssmz399nWGntz56O/edit?usp=sharing&ouid=115826017476220798647&rtpof=true&sd=true\"\n","get_information_about_file_in_drive(url)\n","\n","\n","# -------------------------------\n","# Step 2. Authenticate Google Drive API\n","# -------------------------------\n","auth.authenticate_user() # Google Colab authentication\n","creds, _ = default()  # Get default Colab credentials\n","drive_service = build('drive', 'v3', credentials=creds)\n","\n","\n","# -------------------------------\n","# Step 3. Extract File ID\n","# -------------------------------\n","file_id = get_spreadsheet_id(url)\n","\n","\n","\n","# -------------------------------\n","# Step 4. Download Excel File\n","# -------------------------------\n","request = drive_service.files().get_media(fileId=file_id)\n","fh = io.BytesIO()\n","downloader = MediaIoBaseDownload(fh, request)\n","\n","done = False\n","while not done:\n","    status, done = downloader.next_chunk()\n","    print(f\"ðŸ”„ Download progress: {int(status.progress() * 100)}%\")\n","\n","# Move to beginning of file for openpyxl\n","fh.seek(0)\n","\n","\n","# -------------------------------\n","# Step 5. Load Workbook\n","# -------------------------------\n","wb = openpyxl.load_workbook(fh, data_only=True)\n","\n","\n","\n","# -------------------------------\n","# Step 6. Define Sheet Configurations\n","# Each country has:\n","# - list of columns to use\n","# - list of column names (for renaming)\n","# -------------------------------\n","df_fs = pd.DataFrame()\n","dict_of_countries = {\n","    'USA ': [USA_columns, columns_names_usa],\n","    'CA': [CA_columns, columns_names_other_countries],\n","    'UK ': [UK_columns, columns_names_other_countries],\n","    'EU ': [EU_columns, columns_names_other_countries],\n","    'JP': [JP_columns, columns_names_other_countries]\n","}\n","\n","\n","# -------------------------------\n","# Step 7. Iterate Sheets and Clean Data\n","# -------------------------------\n","\n","for country, (columns_to_use, column_names) in dict_of_countries.items():\n","    ws = wb[country]\n","\n","    # Read sheet rows (skip hidden rows, but keep header row)\n","    data = []\n","    for i, row in enumerate(ws.iter_rows(values_only=True), start=1):\n","        if i == 1 or not ws.row_dimensions[i].hidden:\n","            data.append(row)\n","\n","    # Convert to DataFrame\n","    df_s = pd.DataFrame(data)\n","\n","    # Remove non-standard rows and extra columns\n","    df_s  = df_s.iloc[3:, 0:len(pd.read_excel(fh, header=[0, 1 ], sheet_name= country).columns)] # remove unformal columns and rows\n","    df_s.columns = pd.read_excel(fh, header=[0, 1 ], sheet_name= country).columns # give the columns its original names\n","    df_s = df_s[columns_to_use]  # Keep only needed columns\n","    df_s.columns = column_names  # Rename columns\n","    df_s['Country'] = country    # Add country label\n","    df_s = df_s[df_s['ASIN'].isna() == False ] # keep only the filled columns\n","\n","    # Append to final dataset\n","    df_fs = pd.concat([df_s, df_fs], ignore_index=True)\n","\n","\n","\n","\n","# -------------------------------\n","# Step 8. Final Cleanup\n","# -------------------------------\n","\n","df_fs.fillna(0,inplace = True) # Replace NaN with 0\n","df_fs['Country'] = df_fs['Country'].str.strip().replace({'USA': 'US'})  # Normalize country codes\n","df_fs['ASIN'] = df_fs['ASIN'].str.strip()   # Clean ASIN values\n","\n","\n","\n","\n","# SKU and products name duplication cleaning\n","df_asin_sku = df_fs[['Products', 'ASIN', 'Country', 'SKU']].copy()\n","\n","df_asin_sku_grouped = (\n","    df_asin_sku\n","    .groupby([\"ASIN\", \"Country\"])\n","    .agg({\n","        \"SKU\": lambda x: \", \".join(sorted(set(x.dropna().astype(str)))),\n","        \"Products\": lambda x: \", \".join(sorted(set(x.dropna().astype(str))))\n","    })\n","    .reset_index()\n",")\n","\n","\n","\n","# exclude the out of stock products\n","df_fs = df_fs[df_fs['TOTAL UNITS'] > 0 ]\n","\n","# Pivot for structured analysis (grouped by product, ASIN, and country)\n","df_fs = pd.pivot_table(df_fs, index = ['ASIN', 'Country'], values = ['Average Sales (7 days)',\n","       'Average Sales (30 days)', 'Units in Amazon', 'Transit to AMZ from 3PL',\n","       'Stocks with 3PL', 'TOTAL UNITS',  'Units in AWD'], aggfunc = 'sum').reset_index()\n","\n","df_fs = df_fs.merge(df_asin_sku_grouped, how = 'left', on =[\"ASIN\", \"Country\"])\n"]},{"cell_type":"markdown","source":["### Cleaning the full stock data and make it ready for our report"],"metadata":{"id":"IkRQkakXQepk"}},{"cell_type":"code","execution_count":13,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"q-LO0_7Favs1","executionInfo":{"status":"ok","timestamp":1755498357728,"user_tz":-180,"elapsed":64,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}},"outputId":"35824086-49a6-4ea1-f028-6ee60fa35186"},"outputs":[{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-3158482333.py:18: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n","  .fillna(0)                 # replace NaN with 0\n"]}],"source":["# --- Step 1: Clean numeric fields ---\n","# Replace \"#N/A\" with NaN, then fill missing values with 0 and convert to float for safe numerical calculations\n","df_fs[['Average Sales (30 days)',\n","       'Average Sales (7 days)',\n","       'Stocks with 3PL',\n","       'TOTAL UNITS',\n","       'Transit to AMZ from 3PL',\n","       'Units in AWD',\n","       'Units in Amazon']] = (\n","    df_fs[['Average Sales (30 days)',\n","           'Average Sales (7 days)',\n","           'Stocks with 3PL',\n","           'TOTAL UNITS',\n","           'Transit to AMZ from 3PL',\n","           'Units in AWD',\n","           'Units in Amazon']]\n","    .replace(\"#N/A\", np.nan)   # convert \"#N/A\" â†’ NaN\n","    .fillna(0)                 # replace NaN with 0\n","    .astype(float)             # safely convert to float\n",")\n","\n","# --- Step 2: Calculate stock duration ---\n","# Days stock lasts using 7-day and 30-day average sales\n","df_fs[\"Days stock last (7 days)\"] = df_fs['TOTAL UNITS'] / df_fs['Average Sales (7 days)']\n","df_fs[\"Days stock last (30 days)\"] = df_fs['TOTAL UNITS'] / df_fs['Average Sales (30 days)']\n","\n","# --- Step 3: Handle errors ---\n","# Replace any remaining NaN with 0 and infinite values with 0\n","df_fs.fillna(0, inplace = True)\n","df_fs.replace({np.inf: 0, -np.inf: 0}, inplace = True)\n","\n","\n","# --- Step 4: Map thresholds by country ---\n","# Each country has its own LTSF threshold defined in LTSF_Threshold\n","df_fs['LTSF_Threshold'] = df_fs['Country'].map(LTSF_Threshold)\n","\n","# --- Step 5: Flag products exceeding thresholds ---\n","# Compare stock duration against threshold for both 7-day and 30-day calculations\n","df_fs['7 days LTSF'] = df_fs['Days stock last (7 days)'] > df_fs['LTSF_Threshold']\n","df_fs['30 days LTSF'] = df_fs['Days stock last (30 days)'] > df_fs['LTSF_Threshold']\n","\n","# Create final LTSF flag (True if either condition is met)\n","df_fs['LTSF'] = df_fs['7 days LTSF'] | df_fs['30 days LTSF']\n","\n","# --- Step 6: Filter dataset ---\n","# Keep only rows where LTSF flag is True (i.e., long-term stock exists)\n","df_fs = df_fs[df_fs['LTSF'] == True]"]},{"cell_type":"markdown","source":[">> Now we have the full stock dataframe **df_fs**, which contains the active sales data from the full stock file"],"metadata":{"id":"FVuIxnzCfdOA"}},{"cell_type":"markdown","metadata":{"id":"gfvVxpnIPo7r"},"source":["## Getting the sales of our products"]},{"cell_type":"markdown","source":[">> We obtain this data from the sales full file, which is an aggregated file generated by the collect-and-reprocess notebook."],"metadata":{"id":"LFcdy9Q9gDng"}},{"cell_type":"code","execution_count":14,"metadata":{"id":"lfwVxDHT-gNl","executionInfo":{"status":"ok","timestamp":1755498363720,"user_tz":-180,"elapsed":6018,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}}},"outputs":[],"source":["# Define the root path where the final shaped sales files are stored\n","root_path = r'/content/drive/My Drive/Sales Data/Final Shape of Files'\n","\n","# Change the working directory to the root path\n","os.chdir(root_path)\n","\n","# Define the specific columns we need to import from the sales full file\n","df_sf_cols = ['Date','Marketplace','ASIN','SKU','SalesOrganic', 'SalesSponsoredProducts', 'SalesSponsoredDisplay', 'UnitsOrganic', 'UnitsPPC', 'FBALongTermStorageFee', 'FBAStorageFee', 'FBAPerUnitFulfillmentFee', 'NetProfit']\n","\n","# Load the sales full file into a DataFrame with only the required columns\n","df_sf = pd.read_csv(Sales_full, usecols = df_sf_cols)\n","\n","# Replace missing values (NaN) with 0 for consistency in calculations\n","df_sf.fillna(0, inplace = True)\n","\n","# Convert 'Date' column to datetime format (day/month/year)\n","df_sf['Date'] = pd.to_datetime(df_sf['Date'], format = \"%d/%m/%Y\")\n","\n","# Extract year and month from the 'Date' column for easier grouping/aggregation\n","df_sf['Year'] = df_sf['Date'].dt.year\n","df_sf['Month'] = df_sf['Date'].dt.month_name()\n","\n","# Calculate new business metrics:\n","# Gross Revenue = organic + sponsored products + sponsored display sales\n","df_sf['Gross Revenue'] = df_sf['SalesOrganic'] +  df_sf['SalesSponsoredProducts'] + df_sf['SalesSponsoredDisplay']\n","\n","# Units Sold = organic units + PPC units\n","df_sf['Units Sold'] = df_sf['UnitsOrganic'] + df_sf['UnitsPPC']\n","\n","# FBA storage fee = long-term storage + regular storage + per-unit fulfillment\n","df_sf['FBA storage fee'] = df_sf['FBALongTermStorageFee'] + df_sf['FBAStorageFee'] +  df_sf['FBAPerUnitFulfillmentFee']\n","\n","# Reorder and keep only the final relevant columns for reporting\n","df_sf = df_sf[['Date', 'Marketplace', 'ASIN','SKU',\n","       'Year','Month', 'Gross Revenue', 'Units Sold', 'FBA storage fee', 'NetProfit']]\n","\n","\n"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"r6D3BIhdEw0N","executionInfo":{"status":"ok","timestamp":1755498363740,"user_tz":-180,"elapsed":6,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}}},"outputs":[],"source":["# Filter the sales full dataframe to only include data from the previous month\n","df_sf_last_month = filter_df_by_previous_months(df_sf, num_of_previous_months = 1)\n","\n","# Aggregate Gross Revenue and Net Profit by Marketplace, ASIN, Year, and Month\n","df_sf_last_month  = pd.pivot_table(df_sf_last_month, index = ['Marketplace', 'ASIN', 'Year', 'Month'], values = ['Gross Revenue', 'NetProfit'], aggfunc = 'sum').reset_index()\n","\n","# Map each Marketplace to its corresponding Country using the predefined dictionary\n","df_sf_last_month['Country'] = df_sf_last_month['Marketplace'].map(dict_maketplace_country)\n","\n","# Re-aggregate data by Country and ASIN (summing up revenue and profit)\n","df_sf_last_month = pd.pivot_table(df_sf_last_month, index = ['Country', 'ASIN'], values = ['Gross Revenue', 'NetProfit'], aggfunc = 'sum').reset_index()\n","\n","# Calculate Net Margin = Net Profit Ã· Gross Revenue\n","df_sf_last_month['NetMargin'] = df_sf_last_month['NetProfit'] / df_sf_last_month['Gross Revenue']\n","\n","# Replace missing values with 0 to ensure clean numerical results\n","df_sf_last_month.fillna(0, inplace = True)\n","\n","# Replace infinite values (e.g., division by zero) with 0\n","df_sf_last_month.replace({np.inf: 0, -np.inf: 0}, inplace = True)\n","\n"]},{"cell_type":"markdown","source":[">> Now we have the full sales data of the previous month **df_sf_last_month** which contains the margins of our products in the last month"],"metadata":{"id":"DHO-lS-mhUdT"}},{"cell_type":"markdown","metadata":{"id":"CUEsJhWlP8tx"},"source":["## Getting our products' prices"]},{"cell_type":"markdown","source":[">> We obtain this data from the current price file, which is an aggregated file generated by the collect-and-reprocess notebook."],"metadata":{"id":"fP-e7TVahv97"}},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MTCReHWN-gRQ","executionInfo":{"status":"ok","timestamp":1755498363767,"user_tz":-180,"elapsed":20,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}},"outputId":"914294cf-85be-47ca-c69c-7cf9d7fcaef9"},"outputs":[{"output_type":"stream","name":"stdout","text":["{}\n"]},{"output_type":"stream","name":"stderr","text":["/tmp/ipython-input-4119580494.py:11: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  price_list.rename(columns = {'Region': 'Country'}, inplace = True)\n"]}],"source":["# Read the current price data file\n","df_cp = pd.read_csv(Current_price)\n","\n","# Sort the data so the most recent price entries come first\n","df_cp.sort_values('Date', ascending = False, inplace = True)\n","\n","# Keep only the latest price record for each unique ASINâ€“Regionâ€“Marketplace combination\n","price_list = df_cp.drop_duplicates(subset = ['ASIN','Region', 'Marketplace'], keep = 'first')\n","\n","# Rename 'Region' column to 'Country' for consistency with other datasets\n","price_list.rename(columns = {'Region': 'Country'}, inplace = True)\n","\n","# Keep only the relevant columns\n","price_list = price_list[['Marketplace', 'ASIN', 'Price']]\n","\n","# Map each Marketplace to its corresponding Country\n","price_list['Country'] = price_list['Marketplace'].map(dict_maketplace_country)\n","\n","# Get unmapped marketplaces with their counts\n","print(get_unmapped_marketplaces(price_list, 'Marketplace', dict_maketplace_country, with_counts=True))\n","\n","# Aggregate prices by Country and ASIN (using mean in case multiple records exist)\n","price_list = pd.pivot_table(price_list, index = ['Country', 'ASIN'], values = ['Price'], aggfunc = 'mean').reset_index()"]},{"cell_type":"markdown","source":[">> Now, we have the **price_list** dataframe, which contains the most recent price for each product in every country."],"metadata":{"id":"iwTbzfhFiQ0R"}},{"cell_type":"markdown","metadata":{"id":"fA4blrqnQPEC"},"source":["# Gathering our sources together\n"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sRzGyc9phmtr","executionInfo":{"status":"ok","timestamp":1755498363784,"user_tz":-180,"elapsed":13,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}},"outputId":"715e18c7-9910-48b7-e895-17f4e3917ef3"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["array(['CA', 'UK', 'US', 'EU', 'JP'], dtype=object)"]},"metadata":{},"execution_count":17}],"source":["# ------------------------------------------------------------------\n","# Define the column order for the final dataframe to ensure a consistent structure\n","# ------------------------------------------------------------------\n","cols_order = ['Country',\n","    'Products', 'SKU', 'ASIN',\n","    'Average Sales (7 days)', 'Average Sales (30 days)',\n","    'Units in Amazon', 'Units in AWD',\n","    'Stocks with 3PL', 'Transit to AMZ from 3PL', 'TOTAL UNITS',\n","    'Days stock last (7 days)', 'Days stock last (30 days)',\n","    'Price', 'NetMargin', 'LTSF_Threshold',\n","    '7 days LTSF', '30 days LTSF', 'LTSF',\n","    'Gross Revenue', 'NetProfit'\n","]\n","\n","# ------------------------------------------------------------------\n","# Step 1: Merge the full stock dataframe (df_fs) with the price list\n","# This adds the most recent products' pricing to the stock data\n","# ------------------------------------------------------------------\n","df_map = df_fs.merge(price_list, on=['Country', 'ASIN'], how='left')\n","\n","# ------------------------------------------------------------------\n","# Step 2: Merge the result with last month's sales dataframe (df_sf_last_month)\n","# This produces df_full, which consolidates stock, pricing, and margins data\n","# ------------------------------------------------------------------\n","df_full = df_map.merge(df_sf_last_month, on=['Country', 'ASIN'], how='left').fillna(0)\n","\n","# ------------------------------------------------------------------\n","# Step 3: Handle unavailable values in Price/NetMargin\n","# ------------------------------------------------------------------\n","df_full[[\"NetMargin\", \"Price\"]] = (\n","    df_full[[\"NetMargin\", \"Price\"]].replace({0: \"Unavailable\"})\n",")\n","\n","\n","# ------------------------------------------------------------------\n","# Step 4: Reorder dataframe columns\n","# ------------------------------------------------------------------\n","df_full = df_full[cols_order]\n","\n","# ------------------------------------------------------------------\n","# Step 5: Display available countries\n","# ------------------------------------------------------------------\n","df_full['Country'].unique()\n"]},{"cell_type":"code","source":["# ------------------------------------------------------------------\n","# Save the final stock dataframe (df_full) into an Excel file.\n","# Each country will be written into a separate sheet for easier review.\n","# ------------------------------------------------------------------\n","\n","# Set the root directory where the Excel file will be saved\n","root_path = r'/content/drive/My Drive/Sales Data/Stuck Stocked'\n","os.chdir(root_path)\n","\n","# Write all sheets in one go\n","with pd.ExcelWriter(\"Stuck Stocked.xlsx\", engine=\"openpyxl\") as writer:\n","    # Save the full dataframe as a base sheet\n","    df_full.to_excel(writer, sheet_name=\"All Countries\", index=False)\n","\n","    # Loop through each unique country and save to its own sheet\n","    for country in df_full['Country'].unique():\n","        df_full[df_full['Country'] == country].to_excel(\n","            writer, sheet_name=country, index=False\n","        )\n"],"metadata":{"id":"jlX_VVe7lp4U","executionInfo":{"status":"ok","timestamp":1755498363810,"user_tz":-180,"elapsed":22,"user":{"displayName":"Nabil Ibrahim","userId":"15942015988695033533"}}},"execution_count":18,"outputs":[]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWCtLnd+UlEAWeAA0sGn37"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}